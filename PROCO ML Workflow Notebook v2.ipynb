{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9098744e",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b2fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# for creating extent from centroid\n",
    "from utils import create_extent_from_centroid, produce_geojson\n",
    "\n",
    "# for adding model prediction results to csv file\n",
    "from utils import add_yolov5_conf_scores, add_efficientnet_conf_scores\n",
    "\n",
    "# for nearest neighbour merge\n",
    "import geopandas as gpd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26243a13",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c6b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = \"data/BASIC SCHOOL DATA + GPS.csv\"\n",
    "\n",
    "# params from original csv file\n",
    "original_lat_col_name = \"lat\"\n",
    "original_lon_col_name = \"lon\"\n",
    "original_crs = \"EPSG:4326\"\n",
    "\n",
    "\n",
    "# grid size for downloading school tiles for validation with ML models\n",
    "tile_width = 256\n",
    "tile_height = 256\n",
    "spatial_resolution = 0.6 # in meters\n",
    "\n",
    "\n",
    "# image downloading\n",
    "\n",
    "\n",
    "# adding model prediction results to DataFrame\n",
    "model_1_name = \"efficientnet\" # classification model\n",
    "model_2_name = \"yolov5\" # object detection model\n",
    "\n",
    "model_1_pred_folder = f\"data/{model_1_name}_predictions\" # data/efficientnet_preds\n",
    "model_2_pred_folder = f\"data/{model_2_name}_predictions\" # data/yolov5_preds\n",
    "\n",
    "model_1_conf_col = f\"conf_{model_1_name}\" # \"conf_efficientnet\"\n",
    "model_2_conf_col = f\"conf_{model_2_name}\" # \"conf_yolov5\"\n",
    "\n",
    "\n",
    "# for nearest neighbour merge\n",
    "pred_gjson_dir = \"data/sudan_predictions_centroid_validated_conf_5.geojson\"\n",
    "pred_data_src_crs = \"EPSG:3857\" # crs location points in model prediction file \n",
    "max_distance = 300 # nearest neighbour distance\n",
    "drop_threshold = 0.5\n",
    "\n",
    "# for \"location_type\" column\n",
    "loc_type_gov_dropped = 0\n",
    "loc_type_gov = 1\n",
    "loc_type_ml = 2\n",
    "\n",
    "# for \"source\" column\n",
    "src_gov_id = 1 # id for all points from government csv file is 1\n",
    "src_ml_id = 2  # id for all ML prediction points are 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90cadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file\n",
    "df = pd.read_csv(csv_dir)\n",
    "\n",
    "# create a new column and add row indexes from the original csv file \n",
    "df['original_index'] = df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d076491",
   "metadata": {},
   "source": [
    "## 1. Drop NAN rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e788a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assumes lat / lon columns contain NaN values\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6f112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a separate df for rows with missing lat/lon values\n",
    "df_nan_latlon = df[df[original_lat_col_name].isna() | df[original_lon_col_name].isna()].copy()\n",
    "\n",
    "# DataFrame without missing lat/lon values (drop nan rows for both lat / lon colulmn)\n",
    "df = df[df[original_lat_col_name].notna()]\n",
    "df = df[df[original_lon_col_name].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03029464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  save the file (optional)  \"\"\"\n",
    "\n",
    "# df.to_csv(\"latlon_nan_rows_dropped.csv\", index = False)\n",
    "# df_nan_latlon.to_csv(\"nan_latlon_rows.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323e35",
   "metadata": {},
   "source": [
    "## 2. Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d11514",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assumes there are duplicated lat / lon values in csv file.\n",
    "Drop duplicates keep only the first row.\n",
    "Cause: \n",
    "    - ground team could not locate the exact location of school but is sure there is a school nearby\n",
    "    - same location point could occur multiple times with different school attributes\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525cfe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "df_without_duplicates = df.drop_duplicates(subset = [original_lat_col_name, original_lon_col_name],\n",
    "                                           keep=\"first\"\n",
    "                                          ).copy()\n",
    "\n",
    "print(f\"After dropping duplicates: {len(df_without_duplicates)} rows\")\n",
    "\n",
    "# create a separate DataFrame with only duplicated rows\n",
    "df_duplicated_rows = df[~ df.index.isin(df_without_duplicates.index) ].copy()\n",
    "\n",
    "df = df_without_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7450b8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  Save file (Optional)  \"\"\"\n",
    "\n",
    "# # cleaned csv file (optional)\n",
    "# df.to_csv(\"cleaned.csv\", index = False)\n",
    "\n",
    "# # a separate csv file with only dropped duplicated rows (optional)\n",
    "# df_duplicated_rows.to_csv(\"duplicated_rows.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03776df6",
   "metadata": {},
   "source": [
    "## 3. Create extent from centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feee0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get x, y (lon / lat) values \n",
    "lat_col_values = df[original_lat_col_name]\n",
    "lon_col_values = df[original_lon_col_name]\n",
    "\n",
    "# Calculate four corner points of the grid\n",
    "top, left, bottom, right = create_extent_from_centroid(src_crs = original_crs,\n",
    "                                                       x = lon_col_values,\n",
    "                                                       y = lat_col_values,\n",
    "                                                       grid_width = tile_width,\n",
    "                                                       grid_height = tile_height,\n",
    "                                                       spatial_resolution = spatial_resolution)\n",
    "\n",
    "# add four corner grid points to DataFrame\n",
    "df[\"top\"] = top\n",
    "df[\"left\"] = left\n",
    "df[\"bottom\"] = bottom\n",
    "df[\"right\"] = right\n",
    "\n",
    "# Add a new column: image ids for downloading and validation by ML models\n",
    "df['image_id'] = list(range(1, len(df)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c1b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  Save csv file (optional)  \"\"\"\n",
    "\n",
    "# df.to_csv(\"with_grid_extent.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3359d7",
   "metadata": {},
   "source": [
    "### Optional - generate geojson file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43efab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce_geojson(df = df, \n",
    "#                 crs = original_crs, \n",
    "#                 columns =  [\"image_id\", \"MOEcode\", \"location\", \"lat\", \"lon\", \"top\", \"left\", \"bottom\", \"right\"], \n",
    "#                 save_dir = f\"school_locations_with_{tile_width}_grids_1.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6aa9de",
   "metadata": {},
   "source": [
    "## 4. Data downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f86f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Done in a separate notebook.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5766b708",
   "metadata": {},
   "source": [
    "## 5. Model training & prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f22313",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model training is done separately.\n",
    "Currently, we use two models:\n",
    "    1. EfficientNet (classification model)\n",
    "    2. YOLOv5 (object detection model)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce48cd27",
   "metadata": {},
   "source": [
    "## 6. Add model prediction probability scores to DataFrame\n",
    "- model probability scores on images downloaded using government location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c595369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put NaN values in two model conf score columns\n",
    "df[model_1_conf_col] = np.nan\n",
    "df[model_2_conf_col] = np.nan\n",
    "\n",
    "#####  add model probability scores to DataFrame  #####\n",
    "\n",
    "# add model 1 (EfficientNet) probability scores to DataFrame\n",
    "df = add_efficientnet_conf_scores(original_df = df,\n",
    "                                  predictions_folder = model_1_pred_folder,\n",
    "                                  school_class_id = 1, \n",
    "                                  school_conf_col = model_1_conf_col, \n",
    "                                  image_id_col = \"image_id\")\n",
    "\n",
    "# add model 2 (YOLOv5) probability scores to DataFrame\n",
    "df = add_yolov5_conf_scores(original_df = df, \n",
    "                            predictions_folder = model_2_pred_folder,\n",
    "                            school_conf_col = model_2_conf_col, \n",
    "                            image_id_col = \"image_id\")\n",
    "\n",
    "# replace NaNs with zeros\n",
    "df[model_1_conf_col] = df[model_1_conf_col].fillna(0)\n",
    "df[model_2_conf_col] = df[model_2_conf_col].fillna(0)\n",
    "\n",
    "# reset index\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81473a03",
   "metadata": {},
   "source": [
    "## 7. Nearest neighbour merge\n",
    "Merge before dropping conf = 0 gov points so that we can keep the gov school data attributes\n",
    "\n",
    "3 different location types:\n",
    "  - 0: dropped points\n",
    "  - 1: gov data\n",
    "  - 2: ML prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8251a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two dataframes to be merged\n",
    "gt_df = df\n",
    "pred_df = gpd.read_file(pred_gjson_dir) # centroid data\n",
    "\n",
    "# convert crs to 4326 if source crs is 3857\n",
    "if pred_data_src_crs == \"EPSG:3857\":\n",
    "    pred_df = pred_df.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "print(\"num gov points:\", len(gt_df))\n",
    "print(\"num pred points:\", len(pred_df))\n",
    "\n",
    "# rename gov gt and pred df columns\n",
    "gt_df = gt_df.rename({'image_id': 'gt_image_id'}, \n",
    "                     axis=1)\n",
    "pred_df = pred_df.rename({'image_id': 'pred_image_id'},\n",
    "                         axis=1)\n",
    "\n",
    "# add lat / lon in pred_df (geometry is centroid)\n",
    "pred_df['lat_pred'] = pred_df.geometry.y\n",
    "pred_df['lon_pred'] = pred_df.geometry.x\n",
    "\n",
    "# select columns\n",
    "pred_df = pred_df[ [\"lat_pred\", \"lon_pred\", \"prob\", \"pred_image_id\", \"geometry\"] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecdb2ca",
   "metadata": {},
   "source": [
    "### 7.1. Create a GeoDataFrame for gt csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25227557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create geometry column for gt csv file\n",
    "geometry = gpd.points_from_xy(gt_df.lon, gt_df.lat, crs = original_crs) # 'EPSG:4326'\n",
    "\n",
    "# create geo-dataframe\n",
    "gt_df = gpd.GeoDataFrame(gt_df, \n",
    "                         geometry = geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ec61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  Save geojson files (Optional)  \"\"\"\n",
    "\n",
    "# gt_df.to_file(\"gt.geojson\")\n",
    "# pred_df.to_file(\"pred.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106f18df",
   "metadata": {},
   "source": [
    "### 7.2. Merge - sjoin_nearest ( how = left )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b899600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nearest points to gov points from predictions df\n",
    "# all points from ground truth csv file will be kept\n",
    "\n",
    "nearest_df = gpd.sjoin_nearest(gt_df.to_crs(\"EPSG:3857\"),\n",
    "                               pred_df.to_crs(\"EPSG:3857\"),\n",
    "                               how = 'left', # keep all rows from left df (gov gt csv)\n",
    "                               distance_col = 'dist',\n",
    "                               max_distance = max_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ebb90a",
   "metadata": {},
   "source": [
    "### 7.3. Drop duplicated ML points from nearest neighbour df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa551476",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- one ML point can be associated with multiple gt gov points\n",
    "- drop duplciated ML points\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df that contains only gov points with ML points very nearby\n",
    "nn_gov_points_df = nearest_df.loc[nearest_df['index_right'].notnull()]\n",
    "\n",
    "\n",
    "\"\"\"  sort by distance and drop duplicates  \"\"\"\n",
    "# sort by distance\n",
    "nn_gov_points_df = nn_gov_points_df.sort_values(by=['dist'],\n",
    "                                                ascending=False)\n",
    "\n",
    "# drop duplicates after sorting by distance\n",
    "nn_gov_points_df = nn_gov_points_df.drop_duplicates(\n",
    "    subset=[\"index_right\", \"lat_pred\", \"lon_pred\", \"prob\", \"pred_image_id\"], \n",
    "    keep=\"first\")\n",
    "\n",
    "# keep only first nearest point in gt dataframe, \n",
    "# replace duplicated point rows with null\n",
    "columns_to_nullify = [ \"index_right\", \"lat_pred\", \"lon_pred\", \"prob\", \"pred_image_id\", \"dist\" ]\n",
    "\n",
    "# row, column indexer\n",
    "nearest_df.loc[~nearest_df.index.isin(nn_gov_points_df.index), columns_to_nullify] = np.nan\n",
    "\n",
    "# check if it actually dropped duplicates. e.g. 6632 is correct for sudan.\n",
    "print(\"Total ML points near gov points:\", len(nearest_df[nearest_df['index_right'].notna()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d72e1",
   "metadata": {},
   "source": [
    "### Split the preds df into 2 parts\n",
    "- one with points merged in gt csv file\n",
    "- one with points that weren't merged in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0855f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_indexes_in_nn = list(nearest_df['index_right'])\n",
    "\n",
    "# points that are near gt_df points (i.e. merged points)\n",
    "points_near_gt = pred_df[pred_df.index.isin(pred_indexes_in_nn)]\n",
    "\n",
    "# points that are not near gt points\n",
    "points_away_from_gt = pred_df[~pred_df.index.isin(pred_indexes_in_nn)].copy()\n",
    "\n",
    "print(\"merged in gt df:\", len(points_near_gt))\n",
    "print(\"not merged points:\", len(points_away_from_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abc25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  Save the splitted dataframess (optional)  \"\"\"\n",
    "# ML points near gov points and points away from gov points\n",
    "# points_near_gt.to_file('points_near_gt_df_dist_300.geojson')\n",
    "# points_away_from_gt.to_file('points_away_gt_df_dist_300.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64472b",
   "metadata": {},
   "source": [
    "### Append the ML dataframe to gov dataframe\n",
    "- appends ML points that have not been merged to gov dataframe\n",
    "- these are the new discovery points by the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad7b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add index column\n",
    "points_away_from_gt['index_right'] = list(points_away_from_gt.index)\n",
    "\n",
    "# merge gov and ML points(that are not near any gov points)\n",
    "merged_df = nearest_df.append(points_away_from_gt)\n",
    "\n",
    "# add a new column for location_type. e.g. gov, ML, dropped gov points\n",
    "merged_df['location_type'] = np.nan\n",
    "\n",
    "# rename index_right to pred_df_index\n",
    "merged_df = merged_df.rename({'index_right': 'pred_df_index',\n",
    "                              'lat' : 'lat_gov',\n",
    "                              'lon' : 'lon_gov'},\n",
    "                             axis=1)\n",
    "\n",
    "# drop geometry column\n",
    "merged_df = merged_df.drop([\"geometry\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f12f18",
   "metadata": {},
   "source": [
    "# 8. Final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb2e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add final_lat, final_lon columns in DataFrame (Pick from gov or ML)\n",
    "\n",
    "1. if both gov and ML present, take ML\n",
    "    - e.g. case: Assuming ML is more precise in location\n",
    "    - idea to try: if gov conf is higher than pred conf, keep gov conf\n",
    "        \n",
    "2. only gov data present:\n",
    "    - either YOLO == 0 or EfficientNet == 0\n",
    "    - if both model conf for gov points < threshold --->  drop them\n",
    "    - e.g. case: error points, like linear points, desert areas and water bodies\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add two columns to be used as final lat / lon points\n",
    "merged_df['lat_final'] = np.nan\n",
    "merged_df['lon_final'] = np.nan\n",
    "\n",
    "# Reset index because there are duplicates\n",
    "merged_df = merged_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a75439",
   "metadata": {},
   "source": [
    "### 8.1. if both gov and ML points are present, take ML points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the ml points subset from merged df\n",
    "ml_pred_points_df = merged_df[merged_df['pred_df_index'].notna()].copy()\n",
    "\n",
    "# ML prediction lat / lon\n",
    "lat_pred = ml_pred_points_df['lat_pred']\n",
    "lon_pred = ml_pred_points_df['lon_pred']\n",
    "\n",
    "# index locations for ml prediction points\n",
    "pred_points_indexes = ml_pred_points_df.index\n",
    "\n",
    "# column names to be modified\n",
    "lat_final_col = ['lat_final']\n",
    "lon_final_col = ['lon_final']\n",
    "loc_type_col = ['location_type']\n",
    "\n",
    "# modify final lat / lon columns\n",
    "merged_df.loc[merged_df.index.isin(pred_points_indexes), lat_final_col] = lat_pred\n",
    "merged_df.loc[merged_df.index.isin(pred_points_indexes), lon_final_col] = lon_pred\n",
    "merged_df.loc[merged_df.index.isin(pred_points_indexes), loc_type_col] = loc_type_ml # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df8695d",
   "metadata": {},
   "source": [
    "### 8.2. Create DataFrames with rows where only gov data is present:\n",
    "Create 2 DataFrames:\n",
    "- one with points that are still kept\n",
    "- one with dropped points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1419efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gov points only (rows with no ML prediction points)\n",
    "non_ml_df      = merged_df[merged_df['pred_df_index'].isna()].copy()\n",
    "\n",
    "# df for gov points that will be kept after filtering with conditions\n",
    "kept_points_df = merged_df[merged_df['pred_df_index'].isna()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4355f1",
   "metadata": {},
   "source": [
    "### 8.3. Drop gov points based on conditions\n",
    "Drop based on the following conditions:\n",
    "- both YOLO and EfficientNet is 0 , drop\n",
    "- both model prob score < threshold ,  drop them\n",
    "- either YOLO == 0 or EfficientNet == 0 (**NOT** doing this)\n",
    "    - doesn't always work\n",
    "    - sometimes, EfficientNet conf is high and there's actually a school but YOLO conf is very low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76819234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Conditions:\n",
    "\n",
    "# both YOLO and EfficientNet conf is 0\n",
    "non_ml_df[ ((non_ml_df['conf_efficientnet'] == 0) & (non_ml_df['conf_yolov5'] == 0)) ]\n",
    "\n",
    "# both model conf less than threshold\n",
    "non_ml_df[ (non_ml_df['conf_efficientnet'] < drop_threshold) & (non_ml_df['conf_yolov5'] < drop_threshold) ]\n",
    "\n",
    "# Either YOLO == 0 or EfficientNet == 0\n",
    "non_ml_df[ (non_ml_df['conf_efficientnet'] == 0) | (non_ml_df['conf_yolov5'] == 0) ]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop points based on conditions\n",
    "\n",
    "# condition 1: both YOLO and EfficientNet conf is 0\n",
    "kept_points_df = kept_points_df[ \n",
    "    ~ ((kept_points_df['conf_efficientnet'] == 0) & (kept_points_df['conf_yolov5'] == 0)) \n",
    "]\n",
    "\n",
    "# condition 2: both model conf less than threshold\n",
    "kept_points_df = kept_points_df[ \n",
    "    ~ ((kept_points_df['conf_efficientnet'] < drop_threshold) & (kept_points_df['conf_yolov5'] < drop_threshold))\n",
    "]\n",
    "\n",
    "kept_points_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df for gov points (with no ML points nearby) that will be dropped after filtering with conditions\n",
    "# dropped_points_df =  non_ml_df[ ~ non_ml_df.index.isin(kept_points_df.index)]\n",
    "# dropped_points_df.to_csv(\"dropped_gov_points.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8588b410",
   "metadata": {},
   "source": [
    "### 8.4. Add gov points filtered with conditions (and with no ML points nearby) to final lat / lon column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be5012",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_points_indexes = kept_points_df.index\n",
    "gov_lat_final = kept_points_df['lat_gov']\n",
    "gov_lon_final = kept_points_df['lon_gov']\n",
    "loc_type_col = ['location_type']\n",
    "\n",
    "# put gov lat/lon in lat_final/ on_final columns\n",
    "merged_df.loc[merged_df.index.isin(kept_points_indexes), lat_final_col] = gov_lat_final\n",
    "merged_df.loc[merged_df.index.isin(kept_points_indexes), lon_final_col] = gov_lon_final\n",
    "merged_df.loc[merged_df.index.isin(kept_points_indexes), loc_type_col] = loc_type_gov # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9e7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save before finalizing (can be used for debugging)\n",
    "# merged_df.to_csv('final_data_debug_version.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c838c22b",
   "metadata": {},
   "source": [
    "### 8.5.  Add also the dropped gov points to final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c783d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the df for finalizing\n",
    "final_df = merged_df.copy()\n",
    "\n",
    "# rather than dropping gov points where both model conf < threshold, set their values in \"location_type\" column to zero\n",
    "dropped_gov_points = final_df.loc[final_df['lat_final'].isna()].copy()\n",
    "\n",
    "dropped_indexes = dropped_gov_points.index\n",
    "\n",
    "dropped_lat = dropped_gov_points['lat_gov']\n",
    "dropped_lon = dropped_gov_points['lon_gov']\n",
    "\n",
    "loc_type_col = ['location_type']\n",
    "lat_final_col = ['lat_final']\n",
    "lon_final_col = ['lon_final']\n",
    "\n",
    "final_df.loc[final_df.index.isin(dropped_indexes), lat_final_col] = dropped_lat\n",
    "final_df.loc[final_df.index.isin(dropped_indexes), lon_final_col] = dropped_lon\n",
    "final_df.loc[final_df.index.isin(dropped_indexes), loc_type_col] = loc_type_gov_dropped # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05969652",
   "metadata": {},
   "source": [
    "### 8.6. Add \"source\" column to DataFrame\n",
    "There are 2 values for **\"source\"** column:\n",
    "- 1 = government location point\n",
    "- 2 = ML prediction point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e916aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_col = 'source'\n",
    "\n",
    "# Add \"source\" column to DataFrame\n",
    "final_df[source_col] = np.nan\n",
    "\n",
    "# add government \n",
    "gov_source_list = [loc_type_gov_dropped, loc_type_gov]\n",
    "ml_source_list = [loc_type_ml]\n",
    "\n",
    "##########  add values in \"source\" column  ##########\n",
    "# add gov points\n",
    "final_df.loc[final_df.location_type.isin(gov_source_list), \n",
    "             source_col] = src_gov_id\n",
    "# add ML points\n",
    "final_df.loc[final_df.location_type.isin(ml_source_list), \n",
    "             source_col] = src_ml_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603efe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Source'] = np.where((final_df['source'] ==1), 'Govt', 'ML')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4aa89c",
   "metadata": {},
   "source": [
    "### 8.7. Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "final_df = final_df.reset_index(drop = True)\n",
    "\n",
    "# drop columns\n",
    "columns_to_drop = [\"top\", \"left\", \"bottom\", \"right\", \n",
    "                   \"pred_df_index\", \"lat_pred\", \"lon_pred\", \"prob\", \"pred_image_id\", \"dist\"]\n",
    "final_df = final_df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "# rename lat / lon column\n",
    "final_df = final_df.rename({'lat_final': 'lat',\n",
    "                            'lon_final' : 'lon'},\n",
    "                             axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e4780d",
   "metadata": {},
   "source": [
    "# Save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2245b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"merged_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c8a82",
   "metadata": {},
   "source": [
    "# Add giga ids and temp school_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97f26926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add giga ids\n",
    "\n",
    "list_unique_id = []\n",
    "for i in range(0, len(merged_data)):\n",
    "    uid = str(uuid.uuid3(uuid.NAMESPACE_DNS, str(merged_data['school_nam'][i])  + str(merged_data['lat'][i]) + str(merged_data['lon'][i])))\n",
    "    list_unique_id.append(uid)              \n",
    "merged_data['giga_id_school'] = list_unique_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41b47c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add giga_school_id_seq\n",
    "\n",
    "merged_data['giga_school_id_seq'] = np.arange(len(merged_data)) + 1\n",
    "merged_data['country_code'] = 'SD'\n",
    "list_hex_res = []\n",
    "for i in range(0, len(merged_data)):\n",
    "    h = (\"ML_\" + merged_data['country_code'][0] + \"_\" + merged_data['giga_school_id_seq'].iloc[i].astype('str').zfill(6)) #Based on school agg count \n",
    "    list_hex_res.append(h)\n",
    "merged_data['giga_school_id_seq'] = list_hex_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1cb2700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use giga_school_id_seq as temp school ids for those with no school ids (the ML data points)\n",
    "merged_data['MOEcode'] = merged_data['MOEcode'].fillna(merged_data['giga_school_id_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5393853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv(\"merged_data_v2.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
